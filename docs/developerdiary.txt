=================
Development Diary
=================

Scratching Out a Game Plan
==========================

RSS has been sneaking back into my mind. I loved Google Reader and fell backon 
Feedly but now it's been in my head to make my own cli feed reader. This will
most likely be done with an interactive prompt using Python's ``cmd`` class
than with ``click`` unless I figure out how to integrate the two.

It is a much bigger project that I initially thought. There's a lot of data to
keep track of and no established method of storing the data as far as I can 
tell. There is the OPML standard, but that's pretty generic and only keeps
track of what subscriptions exist, not what has been read.

Thinking ahead on this, I have to list the things I'd like to be able to do::

    >newsboy subscribe rpython https://reddit.com/r/Python/.rss
    ... subscribe to the rss feed for r/Python (assuming that's the right rss
        address

    >newsboy latest rpython
    <idx> <date> Title... <new|read>
    ... lists 10 (or 15, 20...) entry titles

    >newsboy preview 1
    ... print a text preview (first 600 characters or so of the first indexed
        article

    >newsboy open 2
    ... open's the second item in the list in a web browser

    >newsboy mark 3
    ... mark the third item in the list as read
    
    >newsboy all rpython --unread
    ... list all known rpython articles that haven't been marked as read

    >newsboy save 1
    ... saves the first item into permanent storage

A few things about these sketches tells me I need to keep track of a lot of
data. 

    * There is an idea of a LIST, which can be reset at any time
    * Each feed needs to track what has been downloaded and what is new or read
      (and I want to add, possibly, a seen option in there)
    * I can store the latest xml files from each subscription, and then I need
      to store meta data:  when did I last download, what articles have been 
      seen or read, etc.
    * I'm going to need to figure out how to indicate there's an image in the
      article.

I also don't know enough details to ensure I can treet several different feeds
correctly. Fortunately, there is a tool called FeedReader that should make that
part of my life easier.

Playing with ideas
==================


Trying a small bit of code:

.. code-block:: python
    :caption: get_rpython.py

    import feedparser

    target = "http://www.reddit.com/r/Python/.rss"

    d = feedparser.parse(target)
    print(type(d))
    print(d)

Running this in the command line filled up my console and erased the initial
type. Bummer. Let me not print the results, just the type, and see what I get.

What I get is a feedparser.FeedParserDict.

.. code-block:: python
    :caption: get_rpython.py (take 2)

    import json
    import feedparser

    target = "http://www.reddit.com/r/Python/.rss"

    d = feedparser.parse(target)
    print(type(d))
    with open('rpython.rss', 'w') as fp:
        json.dump(d, fp)

This should store a local copy of the data so we can re-read it to explore 
later on. Such as this example:

.. code-block:: python
    :caption: playground.py

    import json
    import code

    with open('rpython.rss', 'r') as fp:
        d = json.load(fp)

    code.interact("Newsboy Playground", local={'d': d})

Running this from the command line loads a Python intrepreter with access to
the data we've previously stored. Unfortunately this gives a plain dictionary
instead of a feedparser.FeedParserDict so accessing elements must be done
by dictionary key access instead of attribute access.

Feedparser consumes the original xml and turns it into a dictionary for us, and
I have to either a) be OK with that or b) roll my own. I'm going with option A.
Running the ``playground`` script and messing about gives me an idea of how
I could make this whole thing work, in a rough-bound-to-be-buggy kind of way.

The loaded dictionary has the following keys::

    * **feed** basic information about the feed
    * **entries** the Entries themselves (see below)
    * **bozo** a flag if something went wrong
    * **href** the URI of the feed
    * **status** the HTTP code (in this case, 301)
    * **encoding** the file encoding (UTF-8 here)
    * **version** the feed type (atom10)
    * **namespaces** a dictionary of XML nampspaces

I don't have to worry about most of these. Feed and Entries are the meat of 
the matter, obviously. The 301 status is a permanent redirect and feedparser
gets the updated url and fetches data and so far I haven't been able to get
the actual RSS address I'm supposed to use.

Each entry in ``d['entries']`` has an ``id`` which is supposed to be unique.
 
Exploring the details a bit more I see this isn't the normal feed I'd be 
pulling, so I turned to my own WordPress blog, hosted by, well, I don't know
who has it at this point.

I now have two items so maybe I should look at managing subscriptions.

And really before that I have to decide how portable I want this. I could save
the data with script in my Dropbox folder on my home computer, but that 
wouldn't be available to me at my work computer. For that I'd need to 
send data files to Dropbox and read from Dropbox. I could also use Google Drive
but I don't have any other off-site storage options, personally.

Maybe it would be easier to not worry about this right now and if I split my
work on this project across multiple machines, they'll have their own datasets
for a while.

but I went ahead and worried and built a dropbox playground:

.. code-block:: python
    :caption: offsite.py

    import dropbox
    import os
    import code

    with open(os.path.expanduser("~/.dropbox_token"), "r") as fp:
        DROPBOX_ACCESS_TOKEN = fp.read()

    dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)
    for entry in dbx.files_list_folder('').entries:
        print(entry.name)

    code.interact('Dropbox Playground', local={'d': dbx})

Once in the playground, I found that this worked:

.. code-block:: python

    >>> with(open('jre.rss', 'rb') as fp:
            d.files_upload(fp.read(), '/jre.news')

Stored the file on Dropbox and gave me a ``FileMetadata`` object with a bunch 
of information I may decide to use.

Suddenly I feel a bit more powerful and a whole lot more confused at the same
time. This is a very personal access and I had to create an App on Dropbox 
and that gave me access to create an access token which I could save. It doesn't
help me get anyone else linking newsboy to Dropbox. I guess we call that a
"todo" under "much much later".

Managing all this Data
======================

Before I really start playing with all the data and samples I've collected,
I need to decide how to store the data, and for that I'm turning to SQLAlchemy,
because I need to learn it, and Alembic for managing migrations, because I
want to know how this works as well.

I need to track:

    #. Subscriptions

       #. URL
       #. Last update time

    #. Articles

       #. id (should be safe to use as a primary key)
       #. status (new, seen, read)
       #. a translation of the article data from feedparser

Here's my thinking: I could pull the xml feeds and store them locally and 
then have feedparser do its magic on each file as needed, but this could lead
to articles being stored multiple times in multiple files. Unless I decide to 
write a massive file-merge tool but that's a distraction here.

Alternatively, I could let feedparser pull the url, create the dictionary
results, then add each result to my articles table. This is probably my best
way forward.

Each entry on my blog has these keys:

    #. title
    #. link
    #. id
    #. author
    #. published_parsed
    #. summary
    #. content[0][value]
    #. tags

I created a simple database based on `this Stack Overflow question`_ to create
an SQLite database with two tables.

.. code-block:: python
    :caption: create_db.py

    import os

    from sqlalchemy import create_engine
    from sqlalchemy import Column, Integer, String
    from sqlalchemy.ext.declarative import declarative_base

    DB_DIR = os.path.join(os.environ['APPDATA'], 'Newsboy')
    if not os.path.isdir(DB_DIR):
        os.mkdir(DB_DIR)
    DB_PATH = os.path.join(DB_DIR, 'newsboy.db')

    engine = create_engine(f'sqlite:///{DB_PATH}', echo=True)
    Base = declarative_base()


    class Article(Base):
        __tablename__ = "article"

        id = Column(Integer, primary_key=True)
        title = Column(String)
        link = Column(String)
        feed_id = Column(String)
        author = Column(String)
        published = Column(String)
        summary = Column(String)


    class Subscription(Base):
        __tablename__ = "feed"

        id = Column(Integer, primary_key=True)
        url = Column(String)
        link = Column(String)


    Base.metadata.create_all(engine)

And then I wrote a simple script that would show me all the artcles in the 
database.

.. code-block:: python
    :caption: list_all_articles.py


    import os
    from sqlalchemy import create_engine

    DB_DIR = os.path.join(os.environ['APPDATA'], 'Newsboy')
    if not os.path.isdir(DB_DIR):
        os.mkdir(DB_DIR)
    DB_PATH = os.path.join(DB_DIR, 'newsboy.db')

    engine = create_engine(f'sqlite:///{DB_PATH}')

    with engine.connect() as con:
        articles = con.execute("SELECT id, title, link FROM article")
        data = articles.fetchall()
        print(data)

Ugly, but at command prompt it gives me an empty list, which is the best of all
possible worlds.
:
Before moving on, though, I want to capture this schema in the migrations,
so running ``alembic revision -m "create article and feed tables"`` may not be
the right way to do things, but it didn't do a spit-take, so I'm good.

Now I need to load articles into the database. I have two .rss files that are
JSON dump files of dictionaries. I created an sql_playground to explore the
engine, and realized that I have no idea how to actually add things outside
of putting SQL statemnets inside an  execute statement.

So maybe what I really need is an up-to-date SQLAlchemy tutorial. Everything
I've found is almost helpful, but it doesn't answer a few key questions:

    * Can I extract the class for a table through the ORM?

Well, that's it, actually. that's where I'm stuck.
 
After watching Mike Bayer's PyCon talk about all of this, I have a hack of a
solution. It seems my project architexture is weak because I don't have a
full understanding of how I'm supposed to set this up. I have the ``create_db``
file which establishes the engine and the ORM classes. Every tutorial I've seen
uses these classes, so I changed ``list_all_articles`` to use them.

.. code-block:: python
    :caption: list_all_articles.py (updated)

    from create_db import engine, Article
    from sqlalchemy.sql import select

    conn = engine.connect()
    s = select([Article])
    print(str(s))
    result = conn.execute(s)
    for row in result.fetchall():
        print(row) 

I still get no results, but now I feel ready to actually load something.


.. _this Stack Overflow question: https://stackoverflow.com/questions/16284537/sqlalchemy-creating-an-sqlite-database-if-it-doesnt-exist


