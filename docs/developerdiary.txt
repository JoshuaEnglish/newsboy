=================
Development Diary
=================

Scratching Out a Game Plan
==========================

RSS has been sneaking back into my mind. I loved Google Reader and fell backon 
Feedly but now it's been in my head to make my own cli feed reader. This will
most likely be done with an interactive prompt using Python's ``cmd`` class
than with ``click`` unless I figure out how to integrate the two.

It is a much bigger project that I initially thought. There's a lot of data to
keep track of and no established method of storing the data as far as I can 
tell. There is the OPML standard, but that's pretty generic and only keeps
track of what subscriptions exist, not what has been read.

Thinking ahead on this, I have to list the things I'd like to be able to do::

    >newsboy subscribe rpython https://reddit.com/r/Python/.rss
    ... subscribe to the rss feed for r/Python (assuming that's the right rss
        address

    >newsboy latest rpython
    <idx> <date> Title... <new|read>
    ... lists 10 (or 15, 20...) entry titles

    >newsboy preview 1
    ... print a text preview (first 600 characters or so of the first indexed
        article

    >newsboy open 2
    ... open's the second item in the list in a web browser

    >newsboy mark 3
    ... mark the third item in the list as read
    
    >newsboy all rpython --unread
    ... list all known rpython articles that haven't been marked as read

    >newsboy save 1
    ... saves the first item into permanent storage

A few things about these sketches tells me I need to keep track of a lot of
data. 

    * There is an idea of a LIST, which can be reset at any time
    * Each feed needs to track what has been downloaded and what is new or read
      (and I want to add, possibly, a seen option in there)
    * I can store the latest xml files from each subscription, and then I need
      to store meta data:  when did I last download, what articles have been 
      seen or read, etc.
    * I'm going to need to figure out how to indicate there's an image in the
      article.

I also don't know enough details to ensure I can treet several different feeds
correctly. Fortunately, there is a tool called FeedReader that should make that
part of my life easier.

Playing with ideas
==================


Trying a small bit of code:

.. code-block:: python
    :caption: get_rpython.py

    import feedparser

    target = "http://www.reddit.com/r/Python/.rss"

    d = feedparser.parse(target)
    print(type(d))
    print(d)

Running this in the command line filled up my console and erased the initial
type. Bummer. Let me not print the results, just the type, and see what I get.

What I get is a feedparser.FeedParserDict.

.. code-block:: python
    :caption: get_rpython.py (take 2)

    import json
    import feedparser

    target = "http://www.reddit.com/r/Python/.rss"

    d = feedparser.parse(target)
    print(type(d))
    with open('rpython.rss', 'w') as fp:
        json.dump(d, fp)

This should store a local copy of the data so we can re-read it to explore 
later on. Such as this example:

.. code-block:: python
    :caption: playground.py

    import json
    import code

    with open('rpython.rss', 'r') as fp:
        d = json.load(fp)

    code.interact("Newsboy Playground", local={'d': d})

Running this from the command line loads a Python intrepreter with access to
the data we've previously stored. Unfortunately this gives a plain dictionary
instead of a feedparser.FeedParserDict so accessing elements must be done
by dictionary key access instead of attribute access.

Feedparser consumes the original xml and turns it into a dictionary for us, and
I have to either a) be OK with that or b) roll my own. I'm going with option A.
Running the ``playground`` script and messing about gives me an idea of how
I could make this whole thing work, in a rough-bound-to-be-buggy kind of way.

The loaded dictionary has the following keys::

    * **feed** basic information about the feed
    * **entries** the Entries themselves (see below)
    * **bozo** a flag if something went wrong
    * **href** the URI of the feed
    * **status** the HTTP code (in this case, 301)
    * **encoding** the file encoding (UTF-8 here)
    * **version** the feed type (atom10)
    * **namespaces** a dictionary of XML nampspaces

I don't have to worry about most of these. Feed and Entries are the meat of 
the matter, obviously. The 301 status is a permanent redirect and feedparser
gets the updated url and fetches data and so far I haven't been able to get
the actual RSS address I'm supposed to use.

Each entry in ``d['entries']`` has an ``id`` which is supposed to be unique.
 
Exploring the details a bit more I see this isn't the normal feed I'd be 
pulling, so I turned to my own WordPress blog, hosted by, well, I don't know
who has it at this point.

I now have two items so maybe I should look at managing subscriptions.
